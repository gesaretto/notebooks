{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions useful for SeinfeldBot\n",
    "\n",
    "## Currently using this lemmatizer to extract only nouns. Let's see how that goes.\n",
    "### Problem is, much of the speech in Seinfeld is heavy on verbs and idioms. Do we really want to exclude all of that?\n",
    "### Maybe we should start with all the parts in place, and then eliminate everything but the nouns to see how that works.\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "from scipy.spatial.distance import cosine \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "english_stopwords = stopwords.words(\"english\") + [\"n't\", \"Ca\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Note: the lemmatizer keeps only nouns, to allow for faster processing and to find more relevant answers.\n",
    "# Can be tweaked by swapping the commented lines.\n",
    "\n",
    "def lemmatize_with_pos(sentence):\n",
    "    # tag_dict = {\"N\": \"n\"}\n",
    "    tag_dict = {\"J\": \"a\", \"R\": \"r\", \"V\": \"v\", \"N\": \"n\"}\n",
    "    lemmatized_sentence = []\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    for token, tag in pos_tag(tokenized_sentence):\n",
    "        if lemmatizer.lemmatize(tag[0].upper()) in tag_dict:\n",
    "            lemmatized = lemmatizer.lemmatize(token, tag_dict[tag[0].upper()])\n",
    "            lemmatized_sentence.append(lemmatized)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# else:\n",
    "#    lemmatized = lemmatizer.lemmatize(token)\n",
    "        \n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    word_regex = \"\\w+\"\n",
    "    preprocessed_sentence = [lemma for lemma in lemmatize_with_pos(sentence) if re.match(word_regex, lemma) and lemma not in english_stopwords]\n",
    "    return preprocessed_sentence\n",
    "\n",
    "def bow_sentence(preprocessed_sentence):\n",
    "    sentence_bow = {lemma for lemma in preprocessed_sentence}\n",
    "    return sentence_bow\n",
    "\n",
    "# This function would find the closest term in the character's BOW based on the Spacy word embeddings for English.\n",
    "# It's very slow, though; so it's currently not in use. \n",
    "\n",
    "def find_closest_in_bow(term, bow, utterances_list):\n",
    "    term_vector = nlp(term).vector\n",
    "    lowest_score = 1.0\n",
    "    closest_word = \"\"\n",
    "    for word in bow:\n",
    "        word_vector = nlp(word).vector\n",
    "        current_cosine = cosine(term_vector, word_vector)\n",
    "        # print(\"...still thinking...\")\n",
    "        # print(current_cosine)\n",
    "        if current_cosine < lowest_score:\n",
    "            print(\"**mumble mumble**\", random.choice(utterances_list), \"**mumble**\")\n",
    "            lowest_score = current_cosine\n",
    "            closest_word = word\n",
    "    return closest_word\n",
    "\n",
    "def compare_bows(bow_1, bow_2):\n",
    "    bag_of_matches = {term for term in bow_1 if term in bow_2}\n",
    "    return (len(bag_of_matches), bag_of_matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
